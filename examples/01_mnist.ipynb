{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04a72c0e",
      "metadata": {
        "id": "04a72c0e"
      },
      "source": [
        "# The Continuous Thought Machine – Tutorial 01: MNIST [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SakanaAI/continuous-thought-machines/blob/main/examples/01_mnist.ipynb) [![arXiv](https://img.shields.io/badge/arXiv-2505.05522-b31b1b.svg)](https://arxiv.org/abs/2505.05522)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d88fc6d1",
      "metadata": {
        "id": "d88fc6d1"
      },
      "source": [
        "Modern deep learning models ignore time as a core computational element. In contrast, the **Continuous Thought Machine (CTM)** introduces internal recurrence and neural synchronization to model *thinking as a temporal process*.\n",
        "\n",
        "### Key Ideas\n",
        "\n",
        "- **Internal Ticks**: The CTM runs over a self-generated temporal axis (independent of input), which we view as a dimension over which thought can unfold.\n",
        "- **Neuron-Level Models**: Each neuron has a private MLP that processes its own history of pre-activations over time.\n",
        "- **Synchronization as Representation**: CTMs compute neuron-to-neuron synchronization over time and use these signals for attention and output.\n",
        "\n",
        "### Why It Matters\n",
        "\n",
        "- Enables **interpretable, dynamic reasoning**\n",
        "- Supports **adaptive compute** (e.g. more ticks for harder tasks)\n",
        "- Works across tasks: classification, reasoning, memory, RL—*without changing the core mechanisms*.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b05cf27b",
      "metadata": {
        "id": "b05cf27b"
      },
      "source": [
        "### MNIST Classification\n",
        "\n",
        "In this tutorial we walk through a simple example; training a CTM to classify MNIST digits. We cover:\n",
        "- Defining the model\n",
        "- Constructing the loss function\n",
        "- Training\n",
        "- Building visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c257dbd3",
      "metadata": {
        "id": "c257dbd3"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapy"
      ],
      "metadata": {
        "id": "OsbF_uGvh5wV",
        "outputId": "9dd3ab60-0cd4-4be3-ee97-1caa6cd20a75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OsbF_uGvh5wV",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapy\n",
            "  Downloading mediapy-1.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from mediapy) (7.34.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapy) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mediapy) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from mediapy) (11.3.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython->mediapy)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy) (2.9.0.post0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython->mediapy) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython->mediapy) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy) (0.2.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapy) (1.17.0)\n",
            "Downloading mediapy-1.2.4-py3-none-any.whl (26 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, mediapy\n",
            "Successfully installed jedi-0.19.2 mediapy-1.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a7bfbfe0",
      "metadata": {
        "id": "a7bfbfe0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.special import softmax\n",
        "import math\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import seaborn as sns\n",
        "import imageio\n",
        "import mediapy\n",
        "\n",
        "import torch._dynamo as dynamo\n",
        "\n",
        "dynamo.config.suppress_errors = False  # Raise errors on fallback"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97220107",
      "metadata": {
        "id": "97220107"
      },
      "source": [
        "Set the seed for reproducability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "88b75133",
      "metadata": {
        "id": "88b75133"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42, deterministic=True):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = deterministic\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5a9f123b",
      "metadata": {
        "id": "5a9f123b"
      },
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee6aa63",
      "metadata": {
        "id": "5ee6aa63"
      },
      "source": [
        "We start by defining some helper classes, which we will use in the CTM.\n",
        "\n",
        "Of note is the SuperLinear class, which implements N unique linear transformations. This SuperLinear class will be used for the neuron-level models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "50c9ac82",
      "metadata": {
        "id": "50c9ac82"
      },
      "outputs": [],
      "source": [
        "class Identity(nn.Module):\n",
        "    \"\"\"Identity Module.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class Squeeze(nn.Module):\n",
        "    \"\"\"Squeeze Module.\"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.squeeze(self.dim)\n",
        "\n",
        "class SuperLinear(nn.Module):\n",
        "    \"\"\"SuperLinear Layer: Implements Neuron-Level Models (NLMs) for the CTM.\"\"\"\n",
        "    def __init__(self, in_dims, out_dims, N, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else Identity()\n",
        "        self.register_parameter('w1', nn.Parameter(\n",
        "            torch.empty((in_dims, out_dims, N)).uniform_(\n",
        "                -1/math.sqrt(in_dims + out_dims),\n",
        "                 1/math.sqrt(in_dims + out_dims)\n",
        "            ), requires_grad=True)\n",
        "        )\n",
        "        self.register_parameter('b1', nn.Parameter(torch.zeros((1, N, out_dims)), requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "            out = self.dropout(x)\n",
        "            out = torch.einsum('BDM,MHD->BDH', out, self.w1) + self.b1\n",
        "            out = out.squeeze(-1)\n",
        "            return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0eb50ea",
      "metadata": {
        "id": "b0eb50ea"
      },
      "source": [
        "Next, we define a helper function `compute_normalized_entropy`. We will use this function inside the CTM to compute the certainty of the model at each internal tick as `certainty = 1 - normalized entropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4eedd9ee",
      "metadata": {
        "id": "4eedd9ee"
      },
      "outputs": [],
      "source": [
        "def compute_normalized_entropy(logits, reduction='mean'):\n",
        "    \"\"\"Computes the normalized entropy for certainty-loss.\"\"\"\n",
        "    preds = F.softmax(logits, dim=-1)\n",
        "    log_preds = torch.log_softmax(logits, dim=-1)\n",
        "    entropy = -torch.sum(preds * log_preds, dim=-1)\n",
        "    num_classes = preds.shape[-1]\n",
        "    max_entropy = torch.log(torch.tensor(num_classes, dtype=torch.float32))\n",
        "    normalized_entropy = entropy / max_entropy\n",
        "    if len(logits.shape)>2 and reduction == 'mean':\n",
        "        normalized_entropy = normalized_entropy.flatten(1).mean(-1)\n",
        "    return normalized_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f89b70a8",
      "metadata": {
        "id": "f89b70a8"
      },
      "source": [
        "## CTM Architecture Overview\n",
        "\n",
        "The CTM implementation is initialized with the following core parameters:\n",
        "\n",
        "- `iterations`: Number of internal ticks (recurrent steps).\n",
        "- `d_model`: Total number of neurons.\n",
        "- `d_input`: Input and attention embedding dimension.\n",
        "- `memory_length`: Length of the sliding activation window used by each neuron.\n",
        "- `heads`: Number of attention heads.\n",
        "- `n_synch_out`: Number of neurons used for output synchronization.\n",
        "- `n_synch_action`: Number of neurons used for computing attention queries.\n",
        "- `out_dims`: Dimensionality of the model's output.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "Upon initialization, the CTM builds the following modules:\n",
        "\n",
        "- **Backbone**: A CNN feature extractor for the input (e.g. image).\n",
        "- **Synapses**: A communication layer allowing neurons to interact.\n",
        "- **Trace Processor**: A neuron-level model that operates on each neuron's temporal activation trace.\n",
        "- **Synchronization Buffers**: For tracking decay.\n",
        "- **Learned Initial States**: Starting activations and traces for the system.\n",
        "\n",
        "---\n",
        "\n",
        "## Forward Pass Mechanics\n",
        "\n",
        "At each internal tick `stepi`, the CTM executes the following procedure:\n",
        "\n",
        "1. **Initialize recurrent state**:\n",
        "    - `state_trace`: Memory trace per neuron.\n",
        "    - `activated_state`: Current post-activations.\n",
        "    - `decay_alpha_out`, `decay_beta_out`: Values for calculating synchronization.\n",
        "\n",
        "2. **Featurize input**:\n",
        "    - Use the CNN backbone to extract key-value attention pairs `kv`.\n",
        "\n",
        "3. **Internal Loop** (for each tick `stepi`):\n",
        "    1. Compute `synchronisation_action` from `n_synch_action` neurons.\n",
        "    2. Generate attention query `q` from this synchronization.\n",
        "    3. Perform multi-head cross-attention over `kv`.\n",
        "    4. Concatenate attention output with the current neuron activations.\n",
        "    5. Update neurons via the synaptic module.\n",
        "    6. Append new activation to the trace window.\n",
        "    7. Update neuron states using the `trace_processor`.\n",
        "    8. Compute `synchronisation_out` from `n_synch_out` neurons.\n",
        "    9. Project to the output space via `output_projector`.\n",
        "    10. Compute prediction certainty from normalized entropy.\n",
        "\n",
        "This inner loop is repeated for the configured number of internal ticks. The CTM emits **predictions and certainties at every internal tick**.\n",
        "\n",
        "> For detailed mathematical formulation of the synchronization mechanism, please refer to the technical report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f357853f",
      "metadata": {
        "id": "f357853f"
      },
      "outputs": [],
      "source": [
        "class ContinuousThoughtMachine(nn.Module):\n",
        "    def __init__(self,\n",
        "                 iterations,\n",
        "                 d_model,\n",
        "                 d_input,\n",
        "                 memory_length,\n",
        "                 heads,\n",
        "                 n_synch_out,\n",
        "                 n_synch_action,\n",
        "                 out_dims,\n",
        "                 memory_hidden_dims,\n",
        "                 dropout=0.0\n",
        "                 ):\n",
        "        super(ContinuousThoughtMachine, self).__init__()\n",
        "\n",
        "        # --- Core Parameters ---\n",
        "        self.iterations = iterations\n",
        "        self.d_model = d_model\n",
        "        self.d_input = d_input\n",
        "        self.memory_length = memory_length\n",
        "        self.n_synch_out = n_synch_out\n",
        "        self.n_synch_action = n_synch_action\n",
        "        self.out_dims = out_dims\n",
        "        self.memory_length = memory_length\n",
        "        self.memory_hidden_dims = memory_hidden_dims\n",
        "\n",
        "        # --- Input Processing  ---\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.LazyConv2d(d_input, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(d_input),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.LazyConv2d(d_input, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(d_input),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "        self.attention = nn.MultiheadAttention(self.d_input, heads, dropout, batch_first=True)\n",
        "        self.kv_proj = nn.Sequential(nn.LazyLinear(self.d_input), nn.LayerNorm(self.d_input))\n",
        "        self.q_proj = nn.LazyLinear(self.d_input)\n",
        "\n",
        "        # --- Core CTM Modules ---\n",
        "        self.synapses = nn.Sequential(\n",
        "                nn.Dropout(dropout),\n",
        "                nn.LazyLinear(d_model * 2),\n",
        "                nn.GLU(),\n",
        "                nn.LayerNorm(d_model)\n",
        "            )\n",
        "        self.trace_processor = nn.Sequential(\n",
        "            SuperLinear(in_dims=memory_length, out_dims=2 * memory_hidden_dims, N=d_model, dropout=dropout),\n",
        "            nn.GLU(),\n",
        "            SuperLinear(in_dims=memory_hidden_dims, out_dims=2, N=d_model, dropout=dropout),\n",
        "            nn.GLU(),\n",
        "            Squeeze(-1)\n",
        "        )\n",
        "\n",
        "        #  --- Start States ---\n",
        "        self.register_parameter('start_activated_state', nn.Parameter(\n",
        "                torch.zeros((d_model)).uniform_(-math.sqrt(1/(d_model)), math.sqrt(1/(d_model))),\n",
        "                requires_grad=True\n",
        "            ))\n",
        "\n",
        "        self.register_parameter('start_trace', nn.Parameter(\n",
        "            torch.zeros((d_model, memory_length)).uniform_(-math.sqrt(1/(d_model+memory_length)), math.sqrt(1/(d_model+memory_length))),\n",
        "            requires_grad=True\n",
        "        ))\n",
        "\n",
        "        # --- Synchronisation ---\n",
        "        self.synch_representation_size_action = (self.n_synch_action * (self.n_synch_action+1))//2\n",
        "        self.synch_representation_size_out = (self.n_synch_out * (self.n_synch_out+1))//2\n",
        "\n",
        "        for synch_type, size in [('action', self.synch_representation_size_action), ('out', self.synch_representation_size_out)]:\n",
        "            print(f\"Synch representation size {synch_type}: {size}\")\n",
        "\n",
        "        self.set_synchronisation_parameters('out')\n",
        "        self.set_synchronisation_parameters('action')\n",
        "\n",
        "        # --- Output Procesing ---\n",
        "        self.output_projector = nn.Sequential(nn.LazyLinear(self.out_dims))\n",
        "\n",
        "    def set_synchronisation_parameters(self, synch_type: str):\n",
        "        synch_representation_size = self.synch_representation_size_action if synch_type == 'action' else self.synch_representation_size_out\n",
        "        self.register_parameter(f'decay_params_{synch_type}', nn.Parameter(torch.zeros(synch_representation_size), requires_grad=True))\n",
        "\n",
        "\n",
        "    def compute_synchronisation(self, activated_state, decay_alpha, decay_beta, r, synch_type):\n",
        "        if synch_type == 'action':\n",
        "            n_synch = self.n_synch_action\n",
        "            selected_left = selected_right = activated_state[:, -n_synch:]\n",
        "        elif synch_type == 'out':\n",
        "            n_synch = self.n_synch_out\n",
        "            selected_left = selected_right = activated_state[:, :n_synch]\n",
        "\n",
        "        outer = selected_left.unsqueeze(2) * selected_right.unsqueeze(1)\n",
        "        i, j = torch.triu_indices(n_synch, n_synch)\n",
        "        pairwise_product = outer[:, i, j]\n",
        "\n",
        "        if decay_alpha is None or decay_beta is None:\n",
        "            decay_alpha = pairwise_product\n",
        "            decay_beta = torch.ones_like(pairwise_product)\n",
        "        else:\n",
        "            decay_alpha = r * decay_alpha + pairwise_product\n",
        "            decay_beta = r * decay_beta + 1\n",
        "\n",
        "        synchronisation = decay_alpha / (torch.sqrt(decay_beta))\n",
        "        return synchronisation, decay_alpha, decay_beta\n",
        "\n",
        "    def compute_features(self, x):\n",
        "        input_features = self.backbone(x)\n",
        "        kv = self.kv_proj(input_features.flatten(2).transpose(1, 2))\n",
        "        return kv\n",
        "\n",
        "    def compute_certainty(self, current_prediction):\n",
        "        ne = compute_normalized_entropy(current_prediction)\n",
        "        current_certainty = torch.stack((ne, 1-ne), -1)\n",
        "        return current_certainty\n",
        "\n",
        "    def forward(self, x, track=False):\n",
        "        B = x.size(0)\n",
        "        device = x.device\n",
        "\n",
        "        # --- Tracking Initialization ---\n",
        "        pre_activations_tracking = []\n",
        "        post_activations_tracking = []\n",
        "        synch_out_tracking = []\n",
        "        synch_action_tracking = []\n",
        "        attention_tracking = []\n",
        "\n",
        "        # --- Featurise Input Data ---\n",
        "        kv = self.compute_features(x)\n",
        "\n",
        "        # --- Initialise Recurrent State ---\n",
        "        state_trace = self.start_trace.unsqueeze(0).expand(B, -1, -1) # Shape: (B, H, T)\n",
        "        activated_state = self.start_activated_state.unsqueeze(0).expand(B, -1) # Shape: (B, H)\n",
        "\n",
        "        # --- Storage for outputs per iteration\n",
        "        predictions = torch.empty(B, self.out_dims, self.iterations, device=device, dtype=x.dtype)\n",
        "        certainties = torch.empty(B, 2, self.iterations, device=device, dtype=x.dtype)\n",
        "\n",
        "        decay_alpha_action, decay_beta_action = None, None\n",
        "        r_action, r_out = torch.exp(-self.decay_params_action).unsqueeze(0).repeat(B, 1), torch.exp(-self.decay_params_out).unsqueeze(0).repeat(B, 1)\n",
        "\n",
        "        _, decay_alpha_out, decay_beta_out = self.compute_synchronisation(activated_state, None, None, r_out, synch_type='out')\n",
        "\n",
        "        # --- Recurrent Loop  ---\n",
        "        for stepi in range(self.iterations):\n",
        "\n",
        "            # --- Calculate Synchronisation for Input Data Interaction ---\n",
        "            synchronisation_action, decay_alpha_action, decay_beta_action = self.compute_synchronisation(activated_state, decay_alpha_action, decay_beta_action, r_action, synch_type='action')\n",
        "\n",
        "            # --- Interact with Data via Attention ---\n",
        "            q = self.q_proj(synchronisation_action).unsqueeze(1)\n",
        "            attn_out, attn_weights = self.attention(q, kv, kv, average_attn_weights=False, need_weights=True)\n",
        "            attn_out = attn_out.squeeze(1)\n",
        "            pre_synapse_input = torch.concatenate((attn_out, activated_state), dim=-1)\n",
        "\n",
        "            # --- Apply Synapses ---\n",
        "            state = self.synapses(pre_synapse_input)\n",
        "            state_trace = torch.cat((state_trace[:, :, 1:], state.unsqueeze(-1)), dim=-1)\n",
        "\n",
        "            # --- Activate ---\n",
        "            activated_state = self.trace_processor(state_trace)\n",
        "\n",
        "            # --- Calculate Synchronisation for Output Predictions ---\n",
        "            synchronisation_out, decay_alpha_out, decay_beta_out = self.compute_synchronisation(activated_state, decay_alpha_out, decay_beta_out, r_out, synch_type='out')\n",
        "\n",
        "            # --- Get Predictions and Certainties ---\n",
        "            current_prediction = self.output_projector(synchronisation_out)\n",
        "            current_certainty = self.compute_certainty(current_prediction)\n",
        "\n",
        "            predictions[..., stepi] = current_prediction\n",
        "            certainties[..., stepi] = current_certainty\n",
        "\n",
        "            # --- Tracking ---\n",
        "            if track:\n",
        "                pre_activations_tracking.append(state_trace[:,:,-1].detach().cpu().numpy())\n",
        "                post_activations_tracking.append(activated_state.detach().cpu().numpy())\n",
        "                attention_tracking.append(attn_weights.detach().cpu().numpy())\n",
        "                synch_out_tracking.append(synchronisation_out.detach().cpu().numpy())\n",
        "                synch_action_tracking.append(synchronisation_action.detach().cpu().numpy())\n",
        "\n",
        "        # --- Return Values ---\n",
        "        if track:\n",
        "            return predictions, certainties, (np.array(synch_out_tracking), np.array(synch_action_tracking)), np.array(pre_activations_tracking), np.array(post_activations_tracking), np.array(attention_tracking)\n",
        "        return predictions, certainties, synchronisation_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a049b6a",
      "metadata": {
        "id": "5a049b6a"
      },
      "source": [
        "## Certainty-Based Loss Function\n",
        "\n",
        "The CTM produces outputs at each internal tick, so the question arises: **how do we optimize the model across this internal temporal dimension?**\n",
        "\n",
        "Our answer is a simple but effective **certainty-based loss** that encourages the model to reason meaningfully across time. Instead of relying on the final tick alone, we aggregate loss from two key internal ticks:\n",
        "\n",
        "1. The tick where the **prediction loss** is lowest.\n",
        "2. The tick where the **certainty** (1 - normalized entropy) is highest.\n",
        "\n",
        "We then take the **average of the losses** at these two points.\n",
        "\n",
        "This approach encourages the CTM to both make accurate predictions and express high confidence in them—while supporting adaptive, interpretable computation over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0f463eb9",
      "metadata": {
        "id": "0f463eb9"
      },
      "outputs": [],
      "source": [
        "def get_loss(predictions, certainties, targets, use_most_certain=True):\n",
        "    \"\"\"use_most_certain will select either the most certain point or the final point.\"\"\"\n",
        "\n",
        "    losses = nn.CrossEntropyLoss(reduction='none')(predictions,\n",
        "                                                   torch.repeat_interleave(targets.unsqueeze(-1), predictions.size(-1), -1))\n",
        "\n",
        "    loss_index_1 = losses.argmin(dim=1)\n",
        "    loss_index_2 = certainties[:,1].argmax(-1)\n",
        "    if not use_most_certain:\n",
        "        loss_index_2[:] = -1\n",
        "\n",
        "    batch_indexer = torch.arange(predictions.size(0), device=predictions.device)\n",
        "    loss_minimum_ce = losses[batch_indexer, loss_index_1].mean()\n",
        "    loss_selected = losses[batch_indexer, loss_index_2].mean()\n",
        "\n",
        "    loss = (loss_minimum_ce + loss_selected)/2\n",
        "    return loss, loss_index_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e54afe0f",
      "metadata": {
        "id": "e54afe0f"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(predictions, targets, where_most_certain):\n",
        "    \"\"\"Calculate the accuracy based on the prediction at the most certain internal tick.\"\"\"\n",
        "    B = predictions.size(0)\n",
        "    device = predictions.device\n",
        "\n",
        "    predictions_at_most_certain_internal_tick = predictions.argmax(1)[torch.arange(B, device=device), where_most_certain].detach().cpu().numpy()\n",
        "    accuracy = (targets.detach().cpu().numpy() == predictions_at_most_certain_internal_tick).mean()\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c1371279",
      "metadata": {
        "id": "c1371279"
      },
      "outputs": [],
      "source": [
        "def prepare_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    train_data = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    test_data = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=256, shuffle=True, num_workers=1)\n",
        "    testloader = torch.utils.data.DataLoader(test_data, batch_size=256, shuffle=True, num_workers=1, drop_last=False)\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2e3fc4d7",
      "metadata": {
        "id": "2e3fc4d7"
      },
      "outputs": [],
      "source": [
        "def update_training_curve_plot(fig, ax1, ax2, train_losses, test_losses, train_accuracies, test_accuracies, steps):\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.clear()\n",
        "    ax1.plot(range(len(train_losses)), train_losses, 'b-', alpha=0.7, label=f'Train Loss: {train_losses[-1]:.3f}')\n",
        "    ax1.plot(steps, test_losses, 'r-', marker='o', label=f'Test Loss: {test_losses[-1]:.3f}')\n",
        "    ax1.set_title('Loss')\n",
        "    ax1.set_xlabel('Step')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax2.clear()\n",
        "    ax2.plot(range(len(train_accuracies)), train_accuracies, 'b-', alpha=0.7, label=f'Train Accuracy: {train_accuracies[-1]:.3f}')\n",
        "    ax2.plot(steps, test_accuracies, 'r-', marker='o', label=f'Test Accuracy: {test_accuracies[-1]:.3f}')\n",
        "    ax2.set_title('Accuracy')\n",
        "    ax2.set_xlabel('Step')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    display(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a492b058",
      "metadata": {
        "id": "a492b058"
      },
      "outputs": [],
      "source": [
        "def train(model, trainloader, testloader, iterations, test_every, device):\n",
        "\n",
        "  optimizer = torch.optim.AdamW(params=list(model.parameters()), lr=0.0001, eps=1e-8)\n",
        "  iterator = iter(trainloader)\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  train_accuracies = []\n",
        "  test_accuracies = []\n",
        "  steps = []\n",
        "\n",
        "  plt.ion()\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "  with tqdm(total=iterations, initial=0, dynamic_ncols=True) as pbar:\n",
        "      test_loss = None\n",
        "      test_accuracy = None\n",
        "      for stepi in range(iterations):\n",
        "\n",
        "          try:\n",
        "              inputs, targets = next(iterator)\n",
        "          except StopIteration:\n",
        "              iterator = iter(trainloader)\n",
        "              inputs, targets = next(iterator)\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          predictions, certainties, _ = model(inputs, track=False)\n",
        "          train_loss, where_most_certain = get_loss(predictions, certainties, targets)\n",
        "          train_accuracy = calculate_accuracy(predictions, targets, where_most_certain)\n",
        "\n",
        "          train_losses.append(train_loss.item())\n",
        "          train_accuracies.append(train_accuracy)\n",
        "\n",
        "          train_loss.backward()\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "\n",
        "          if stepi % test_every == 0 or stepi == iterations - 1:\n",
        "            model.eval()\n",
        "            with torch.inference_mode():\n",
        "                all_test_predictions = []\n",
        "                all_test_targets = []\n",
        "                all_test_where_most_certain = []\n",
        "                all_test_losses = []\n",
        "\n",
        "                for inputs, targets in testloader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    predictions, certainties, _ = model(inputs, track=False)\n",
        "                    test_loss, where_most_certain = get_loss(predictions, certainties, targets)\n",
        "                    all_test_losses.append(test_loss.item())\n",
        "\n",
        "                    all_test_predictions.append(predictions)\n",
        "                    all_test_targets.append(targets)\n",
        "                    all_test_where_most_certain.append(where_most_certain)\n",
        "\n",
        "                all_test_predictions = torch.cat(all_test_predictions, dim=0)\n",
        "                all_test_targets = torch.cat(all_test_targets, dim=0)\n",
        "                all_test_where_most_certain = torch.cat(all_test_where_most_certain, dim=0)\n",
        "\n",
        "                test_accuracy = calculate_accuracy(all_test_predictions, all_test_targets, all_test_where_most_certain)\n",
        "                test_loss = sum(all_test_losses) / len(all_test_losses)\n",
        "\n",
        "                test_losses.append(test_loss)\n",
        "                test_accuracies.append(test_accuracy)\n",
        "                steps.append(stepi)\n",
        "            model.train()\n",
        "\n",
        "            update_training_curve_plot(fig, ax1, ax2, train_losses, test_losses, train_accuracies, test_accuracies, steps)\n",
        "\n",
        "          pbar.set_description(f'Train Loss: {train_loss:.3f}, Train Accuracy: {train_accuracy:.3f} Test Loss: {test_loss:.3f}, Test Accuracy: {test_accuracy:.3f}')\n",
        "          pbar.update(1)\n",
        "\n",
        "  plt.ioff()\n",
        "  plt.close(fig)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2d1658a9",
      "metadata": {
        "id": "2d1658a9",
        "outputId": "2e55205a-9cd3-4d21-8163-9a76c1d17a19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.0MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 488kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.16MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synch representation size action: 136\n",
            "Synch representation size out: 136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 343,658\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "trainloader, testloader = prepare_data()\n",
        "\n",
        "model = ContinuousThoughtMachine(\n",
        "    iterations=15,\n",
        "    d_model=128,\n",
        "    d_input=128,\n",
        "    memory_length=10,\n",
        "    heads=2,\n",
        "    n_synch_out=16,\n",
        "    n_synch_action=16,\n",
        "    memory_hidden_dims=8,\n",
        "    out_dims=10,\n",
        "    dropout=0.0\n",
        ").to(device)\n",
        "\n",
        "sample_batch = next(iter(trainloader))\n",
        "dummy_input = sample_batch[0][:1].to(device)\n",
        "with torch.no_grad():\n",
        "    _ = model(dummy_input)\n",
        "\n",
        "# Now compile the model\n",
        "model = torch.compile(model)\n",
        "\n",
        "def clamp_decay_params(module, _input):\n",
        "    with torch.no_grad():\n",
        "        module.decay_params_action.data.clamp_(0, 15)\n",
        "        module.decay_params_out.data.clamp_(0, 15)\n",
        "\n",
        "model.register_forward_pre_hook(clamp_decay_params)\n",
        "\n",
        "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6d7f20",
      "metadata": {
        "id": "ce6d7f20",
        "outputId": "e35fd6ac-dbb1-431f-d409-88e8c82cee83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10000 [00:00<?, ?it/s]W1126 18:59:33.566000 355 torch/utils/cpp_extension.py:117] [0/0] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ],
      "source": [
        "model = train(model=model, trainloader=trainloader, testloader=testloader, iterations=10000, test_every=500, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7e308f",
      "metadata": {
        "id": "af7e308f"
      },
      "source": [
        "## Visualizing CTM Dynamics\n",
        "\n",
        "We define a function to create GIFs that show how the CTMs dynamics. These visualizations include:\n",
        "\n",
        "- **Neuron activations** at each internal tick  \n",
        "- **Attention patterns** across the input  \n",
        "- **Predictions and certainty** at every step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3fbae96",
      "metadata": {
        "id": "b3fbae96"
      },
      "outputs": [],
      "source": [
        "def make_gif(predictions, certainties, targets, pre_activations, post_activations, attention, inputs_to_model, filename):\n",
        "    def reshape_attention_weights(attention, target_size=28):\n",
        "        # The num_positions will not be a perfect square if the input size is not a perfect square. If d_input is not a perfect sqaure, interpolate\n",
        "        T, B, num_heads, _, num_positions = attention.shape\n",
        "        attention = torch.tensor(attention, dtype=torch.float32).mean(dim=2).squeeze(2)\n",
        "        height = int(num_positions**0.5)\n",
        "        while num_positions % height != 0: height -= 1\n",
        "        width = num_positions // height\n",
        "        attention = attention.view(T, B, height, width)\n",
        "        return F.interpolate(attention, size=(target_size, target_size), mode='bilinear', align_corners=False)\n",
        "\n",
        "    batch_index = 0\n",
        "    n_neurons_to_visualise = 16\n",
        "    figscale = 0.28\n",
        "    n_steps = len(pre_activations)\n",
        "    heCTMap_cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "    frames = []\n",
        "\n",
        "    attention = reshape_attention_weights(attention)\n",
        "\n",
        "    these_pre_acts = pre_activations[:, batch_index, :]\n",
        "    these_post_acts = post_activations[:, batch_index, :]\n",
        "    these_inputs = inputs_to_model[batch_index,:, :, :]\n",
        "    these_attention_weights = attention[:, batch_index, :, :]\n",
        "    these_predictions = predictions[batch_index, :, :]\n",
        "    these_certainties = certainties[batch_index, :, :]\n",
        "    this_target = targets[batch_index]\n",
        "\n",
        "    class_labels = [str(i) for i in range(these_predictions.shape[0])]\n",
        "\n",
        "    mosaic = [['img_data', 'img_data', 'attention', 'attention', 'probs', 'probs', 'probs', 'probs'] for _ in range(2)] + \\\n",
        "             [['img_data', 'img_data', 'attention', 'attention', 'probs', 'probs', 'probs', 'probs'] for _ in range(2)] + \\\n",
        "             [['certainty'] * 8] + \\\n",
        "             [[f'trace_{ti}'] * 8 for ti in range(n_neurons_to_visualise)]\n",
        "\n",
        "    for stepi in tqdm(range(n_steps), desc=\"Processing steps\", unit=\"step\"):\n",
        "        fig_gif, axes_gif = plt.subplot_mosaic(mosaic=mosaic, figsize=(31*figscale*8/4, 76*figscale))\n",
        "        probs = softmax(these_predictions[:, stepi])\n",
        "        colors = [('g' if i == this_target else 'b') for i in range(len(probs))]\n",
        "\n",
        "        axes_gif['probs'].bar(np.arange(len(probs)), probs, color=colors, width=0.9, alpha=0.5)\n",
        "        axes_gif['probs'].set_title('Probabilities')\n",
        "        axes_gif['probs'].set_xticks(np.arange(len(probs)))\n",
        "        axes_gif['probs'].set_xticklabels(class_labels, fontsize=24)\n",
        "        axes_gif['probs'].set_yticks([])\n",
        "        axes_gif['probs'].tick_params(left=False, bottom=False)\n",
        "        axes_gif['probs'].set_ylim([0, 1])\n",
        "        for spine in axes_gif['probs'].spines.values():\n",
        "            spine.set_visible(False)\n",
        "        axes_gif['probs'].tick_params(left=False, bottom=False)\n",
        "        axes_gif['probs'].spines['top'].set_visible(False)\n",
        "        axes_gif['probs'].spines['right'].set_visible(False)\n",
        "        axes_gif['probs'].spines['left'].set_visible(False)\n",
        "        axes_gif['probs'].spines['bottom'].set_visible(False)\n",
        "\n",
        "        # Certainty\n",
        "        axes_gif['certainty'].plot(np.arange(n_steps), these_certainties[1], 'k-', linewidth=2)\n",
        "        axes_gif['certainty'].set_xlim([0, n_steps-1])\n",
        "        axes_gif['certainty'].axvline(x=stepi, color='black', linewidth=1, alpha=0.5)\n",
        "        axes_gif['certainty'].set_xticklabels([])\n",
        "        axes_gif['certainty'].set_yticklabels([])\n",
        "        axes_gif['certainty'].grid(False)\n",
        "        for spine in axes_gif['certainty'].spines.values():\n",
        "            spine.set_visible(False)\n",
        "\n",
        "        # Neuron Traces\n",
        "        for neuroni in range(n_neurons_to_visualise):\n",
        "            ax = axes_gif[f'trace_{neuroni}']\n",
        "            pre_activation = these_pre_acts[:, neuroni]\n",
        "            post_activation = these_post_acts[:, neuroni]\n",
        "            ax_pre = ax.twinx()\n",
        "\n",
        "            ax_pre.plot(np.arange(n_steps), pre_activation, color='grey', linestyle='--', linewidth=1, alpha=0.4)\n",
        "            color = 'blue' if neuroni % 2 else 'red'\n",
        "            ax.plot(np.arange(n_steps), post_activation, color=color, linewidth=2, alpha=1.0)\n",
        "\n",
        "            ax.set_xlim([0, n_steps-1])\n",
        "            ax_pre.set_xlim([0, n_steps-1])\n",
        "            ax.set_ylim([np.min(post_activation), np.max(post_activation)])\n",
        "            ax_pre.set_ylim([np.min(pre_activation), np.max(pre_activation)])\n",
        "\n",
        "            ax.axvline(x=stepi, color='black', linewidth=1, alpha=0.5)\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.grid(False)\n",
        "            ax_pre.set_xticklabels([])\n",
        "            ax_pre.set_yticklabels([])\n",
        "            ax_pre.grid(False)\n",
        "\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_visible(False)\n",
        "            for spine in ax_pre.spines.values():\n",
        "                spine.set_visible(False)\n",
        "\n",
        "        # Input image\n",
        "        this_image = these_inputs[0]\n",
        "        this_image = (this_image - this_image.min()) / (this_image.max() - this_image.min() + 1e-8)\n",
        "        axes_gif['img_data'].set_title('Input Image')\n",
        "        axes_gif['img_data'].imshow(this_image, cmap='binary', vmin=0, vmax=1)\n",
        "        axes_gif['img_data'].axis('off')\n",
        "\n",
        "        # Attention\n",
        "        this_input_gate = these_attention_weights[stepi]\n",
        "        gate_min, gate_max = np.nanmin(this_input_gate), np.nanmax(this_input_gate)\n",
        "        if not np.isclose(gate_min, gate_max):\n",
        "            normalized_gate = (this_input_gate - gate_min) / (gate_max - gate_min + 1e-8)\n",
        "        else:\n",
        "            normalized_gate = np.zeros_like(this_input_gate)\n",
        "        attention_weights_heCTMap = heCTMap_cmap(normalized_gate)[:,:,:3]\n",
        "\n",
        "        axes_gif['attention'].imshow(attention_weights_heCTMap, vmin=0, vmax=1)\n",
        "        axes_gif['attention'].axis('off')\n",
        "        axes_gif['attention'].set_title('Attention')\n",
        "\n",
        "        fig_gif.tight_layout()\n",
        "        canvas = fig_gif.canvas\n",
        "        canvas.draw()\n",
        "        image_numpy = np.frombuffer(canvas.buffer_rgba(), dtype='uint8')\n",
        "        image_numpy = image_numpy.reshape(*reversed(canvas.get_width_height()), 4)[:, :, :3]\n",
        "        frames.append(image_numpy)\n",
        "        plt.close(fig_gif)\n",
        "\n",
        "\n",
        "    mediapy.show_video(frames, width=400, codec=\"gif\")\n",
        "    imageio.mimsave(filename, frames, fps=5, loop=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20040db6",
      "metadata": {
        "id": "20040db6"
      },
      "source": [
        "The top row of the gif shows the input image, the attention weights and the models predictions.\n",
        "\n",
        "The second plot, in black, shows the models certainty over time.\n",
        "\n",
        "The red and blue lines correspond to the post-activations of different neurons, with the corresponding pre-activation shown in gray."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6EzvgPUhrs7"
      },
      "outputs": [],
      "source": [
        "logdir = f\"mnist_logs\"\n",
        "if not os.path.exists(logdir):\n",
        "    os.makedirs(logdir)\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    inputs, targets = next(iter(testloader))\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    predictions, certainties, (synch_out_tracking, synch_action_tracking), \\\n",
        "    pre_activations_tracking, post_activations_tracking, attention = model(inputs, track=True)\n",
        "\n",
        "    make_gif(\n",
        "        predictions.detach().cpu().numpy(),\n",
        "        certainties.detach().cpu().numpy(),\n",
        "        targets.detach().cpu().numpy(),\n",
        "        pre_activations_tracking,\n",
        "        post_activations_tracking,\n",
        "        attention,\n",
        "        inputs.detach().cpu().numpy(),\n",
        "        f\"{logdir}/prediction.gif\"\n",
        "    )"
      ],
      "id": "r6EzvgPUhrs7"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8zvIxk3hyVI"
      },
      "id": "W8zvIxk3hyVI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}